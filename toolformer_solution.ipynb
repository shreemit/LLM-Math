{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project 3 will have two parts - a) The first part involves Automated Prompt Engineering where you get to build the use of a Calculator Tool automatically for \"Math questions\" asked of an LLM. You can compare the evaluation performance with/without the use of the tool and see how it can improve the performance of GPT 3.5. b) The second part involves the use of Stability AI APIs to work on different image editing tasks - Including \"Removing the photobomber\" or \"Removing artifacts from an image\" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: This part aims to assess the mathematical reasoning capabilities of Large Language Models (LLMs), specifically focusing on zero-shot learning, few-shot in-context learning, and their ability to integrate external tools when solving arithmetic word problems from the SVAMP dataset. The SVAMP dataset comprises simple variations of arithmetic math word problems up to grade 4, designed to test the models beyond mere pattern recognition and evaluate their genuine problem-solving skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The SVAMP dataset (Simple Variations on Arithmetic Math word Problems) represents a specialized challenge set designed to assess the capabilities of state-of-the-art (SOTA) models in solving arithmetic word problems of up to grade 4 level. \n",
    "#### Unlike conventional benchmarks, which models often solve by exploiting simple heuristics, SVAMP introduces a series of one-unknown math word problems crafted to underscore the limitations of these models, demonstrating their struggle to solve even elementary problems effectively. \n",
    "#### The challenge is two fold: (1) identify what numbers in the context are actually used for the question and construct a math equation (2) correctly compute the math equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ChilleD/SVAMP\")\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "# https://huggingface.co/datasets/ChilleD/SVAMP?row=0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ChatGPT completion\"\"\"\n",
    "import os\n",
    "def chatgpt_completion(prompt_text):\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    messages = [\n",
    "    { \"role\": \"user\", \"content\": prompt_text },\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        max_tokens=1000,\n",
    "        top_p=1,)\n",
    "    response_text = response.choices[0].message.content\n",
    "    return response_text\n",
    "\n",
    "\n",
    "def evaluate(results, answers):\n",
    "    accs = []\n",
    "    for result, answer in zip(results, answers):\n",
    "        try:\n",
    "            _ = float(result)\n",
    "            if float(result) == float(answer):\n",
    "                accs.append(1)\n",
    "            else:\n",
    "                accs.append(0)\n",
    "        except:\n",
    "            accs.append(0)\n",
    "\n",
    "    print(\"The scores on the test set: \", sum(accs)/len(accs)) # around 38% - 50%\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# time to restrict query speed\n",
    "class SpeedLimitTimer:\n",
    "    def __init__(self, second_per_step=3.1):\n",
    "        self.record_time = time.time()\n",
    "        self.second_per_step = second_per_step\n",
    "\n",
    "    def step(self):\n",
    "        time_div = time.time() - self.record_time\n",
    "        if time_div <= self.second_per_step:\n",
    "            time.sleep(self.second_per_step - time_div)\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def sleep(self, s):\n",
    "        time.sleep(s)\n",
    "\n",
    "\n",
    "timer = SpeedLimitTimer(second_per_step=3.1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Zero Shot Prompting LLM (i.e. ChatGPT)\n",
    "#### In this part, we are going to ask ChatGPT to directly answer the math questions without using any tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_zeroshot_chatgpt_output(chatgpt_output):\n",
    "    # Follow Toolformer \n",
    "    # https://arxiv.org/abs/2302.04761\n",
    "    # Zero-Shot Math Reasoning with ChatGPT     \n",
    "    # Check for the first number predicted by the model. \n",
    "    # An exception to this is if the model’s prediction contains an equation (e.g., “The correct answer is 5+3=8”), in which case we consider the first number after the “=” sign to be its prediction.\n",
    "    def is_number(s):\n",
    "        try:\n",
    "            float(s)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    if \"=\" in chatgpt_output:\n",
    "        chatgpt_output = chatgpt_output.split(\"=\")[1]\n",
    "\n",
    "    words = chatgpt_output.split()\n",
    "    for word in words:\n",
    "        if is_number(word):\n",
    "            return word\n",
    "    return float(\"inf\")  # Return inf if no number word is found\n",
    "\n",
    "def zero_shot_prompt_chatgpt(test_dataset):\n",
    "    results = []\n",
    "    answers = []\n",
    "    for data in tqdm(test_dataset):\n",
    "        context = data[\"Body\"]    \n",
    "        question = data[\"Question\"] \n",
    "        answer = data[\"Answer\"]\n",
    "        answers.append(answer)\n",
    "\n",
    "        example_prompt =  context + \" \" + question \n",
    "        chatgpt_output = chatgpt_completion(example_prompt)\n",
    "        result = parse_zeroshot_chatgpt_output(chatgpt_output)\n",
    "        results.append(result)\n",
    "        timer.step()\n",
    "    return results, answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [15:35<00:00,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scores on the test set:  0.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "zeroshot_results, answers = zero_shot_prompt_chatgpt(test_dataset) # 47 # gpt-3.5-turbo-0125\n",
    "evaluate(zeroshot_results, answers)  # This should give you scores 38-50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Instrution following and tool use with LLM  (i.e. ChatGPT)\n",
    "#### You will see that zero shot chatgpt performance is less than ideal. Next we are gonna leverage the tool use capabilities of LLM.\n",
    "#### In this case here, it's the abilitiy to leverage external calculator to deal with math questions. \n",
    "#### Using Toolformer as a reference, we construct a instruction prompt and also put in a few {input:output} demonstrations to guide ChatGPT to generate the output format we want.\n",
    "#### You will also be asked to implement some functions to finish the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOLFORMER_CALCULATOR_PROMPT_4Shot = \"\"\"\n",
    "Your task is to add calls to a\n",
    "Calculator API to a piece of text.\n",
    "The calls should help you get\n",
    "information required to complete the\n",
    "text. You can call the API by writing\n",
    "\"[Calculator(expression)]\" where\n",
    "\"expression\" is the expression to be\n",
    "computed. Here are some examples of API\n",
    "calls:\n",
    "Input: The number in the next term is 18\n",
    "+ 12 x 3 = 54.\n",
    "Output: The number in the next term is\n",
    "18 + 12 x 3 = [Calculator(18 + 12 * 3)]\n",
    "54.\n",
    "Input: The population is 658,893 people.\n",
    "This is 11.4% of the national average of\n",
    "5,763,868 people.\n",
    "Output: The population is 658,893 people.\n",
    "This is 11.4% of the national average of\n",
    "[Calculator(658,893 / 11.4%)] 5,763,868\n",
    "people.\n",
    "Input: A total of 252 qualifying matches\n",
    "were played, and 723 goals were scored\n",
    "(an average of 2.87 per match). This is\n",
    "three times less than the 2169 goals\n",
    "last year.\n",
    "Output: A total of 252 qualifying\n",
    "matches were played, and 723 goals were\n",
    "scored (an average of [Calculator(723\n",
    "/ 252)] 2.87 per match). This is twenty\n",
    "goals more than the [Calculator(723 -\n",
    "20)] 703 goals last year.\n",
    "Input: I went to Paris in 1994 and\n",
    "stayed there until 2011, so in total,\n",
    "it was 17 years.\n",
    "Output: I went to Paris in 1994 and\n",
    "stayed there until 2011, so in total, it\n",
    "was [Calculator(2011 - 1994)] 17 years.\n",
    "\"\"\" # Prompt from Toolformer \n",
    "# # https://arxiv.org/abs/2302.04761  ### 68%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOLFORMER_CALCULATOR_PROMPT_1Shot = \"\"\"\n",
    "Your task is to add calls to a\n",
    "Calculator API to a piece of text.\n",
    "The calls should help you get\n",
    "information required to complete the\n",
    "text. You can call the API by writing\n",
    "\"[Calculator(expression)]\" where\n",
    "\"expression\" is the expression to be\n",
    "computed. Here are some examples of API\n",
    "calls:\n",
    "Input: The number in the next term is 18\n",
    "+ 12 x 3 = 54.\n",
    "Output: The number in the next term is\n",
    "18 + 12 x 3 = [Calculator(18 + 12 * 3)]\n",
    "54.\n",
    "\"\"\" # Prompt from Toolformer \n",
    "# https://arxiv.org/abs/2302.04761  ### 64%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def calculator(chatgpt_output):\n",
    "    ######################### Put your code here #########################\n",
    "    # In this function, you need to implement a calculator that will compute the equation produced by ChatGPT and reurn a number as the final answer. \n",
    "    # Hint: look out for percentage sign % and dolar sign $.  \n",
    "    if '%' in chatgpt_output:\n",
    "        chatgpt_output = chatgpt_output.replace('%', '/100')\n",
    "    if '$' in chatgpt_output:\n",
    "        chatgpt_output = chatgpt_output.replace('$', '')\n",
    "\n",
    "    try:\n",
    "        # Evaluate the arithmetic expression\n",
    "        result = eval(chatgpt_output)\n",
    "    except Exception as e:\n",
    "        return f\"Error evaluating expression: {e}\"\n",
    "    \n",
    "    return result\n",
    "    ######################### Put your code here #########################\n",
    "    \n",
    "# You can use the below test case to check your implementation.\n",
    "\n",
    "# # Example inputs\n",
    "# example_inputs = [\n",
    "#     \"18 + 12 * 3\",\n",
    "#     \"658893 / 11.4%\",\n",
    "#     \"723 / 252\"\n",
    "# ]\n",
    "\n",
    "# for example in example_inputs:\n",
    "#     result = calculator(example)\n",
    "#     print(result)\n",
    "\n",
    "# 54\n",
    "# 577.9763157894737\n",
    "# 2.869047619047619"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toolformer_prompt_chatgpt(test_dataset, PROMPT):\n",
    "    results = []\n",
    "    answers = []\n",
    "    for idx, data in enumerate(test_dataset):\n",
    "        context = data[\"Body\"]    \n",
    "        question = data[\"Question\"]\n",
    "        answer = data[\"Answer\"]\n",
    "\n",
    "        example_prompt =  PROMPT + context + \" \" + question \n",
    "        try:\n",
    "            response = chatgpt_completion(example_prompt)\n",
    "            print(response)\n",
    "            calculate_out = response.split(\"[Calculator(\")[1].split(\")\")[0]\n",
    "            result = calculator(calculate_out)\n",
    "        except:\n",
    "            result =  float(\"inf\")\n",
    "\n",
    "        results.append(result)\n",
    "        answers.append(answer)\n",
    "\n",
    "        if \n",
    "        \n",
    "        timer.step()\n",
    "    return results, answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset1 = test_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mzero_shot_prompt_chatgpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 47 # gpt-3.5-turbo-0125\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 26\u001b[0m, in \u001b[0;36mzero_shot_prompt_chatgpt\u001b[0;34m(test_dataset)\u001b[0m\n\u001b[1;32m     24\u001b[0m answers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m tqdm(test_dataset):\n\u001b[0;32m---> 26\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBody\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m    \n\u001b[1;32m     27\u001b[0m     question \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion\u001b[39m\u001b[38;5;124m\"\u001b[39m] \n\u001b[1;32m     28\u001b[0m     answer \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary is baking a cake. The recipe calls for 6 cups of flour, 8 cups of sugar, and 7 cups of salt. She already put in 5 cups of flour. To find out how many more cups of sugar than cups of salt she needs to add now, we can use the following calculation:\n",
      "\n",
      "[Calculator(8 - 7)] cups more sugar than salt needs to be added now.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1], [1.0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toolformer_prompt_chatgpt(test_dataset, TOOLFORMER_CALCULATOR_PROMPT_4Shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ChatGPT performance with tool use when prompted with instruction and 4 task demonstrations. This should give you scores 55-70%\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m fourshot_results, answers \u001b[38;5;241m=\u001b[39m \u001b[43mtoolformer_prompt_chatgpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTOOLFORMER_CALCULATOR_PROMPT_4Shot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m evaluate(fourshot_results, answers)   \u001b[38;5;66;03m# run 1 64.66,  run 2 65.33, run 3 65\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[32], line 19\u001b[0m, in \u001b[0;36mtoolformer_prompt_chatgpt\u001b[0;34m(test_dataset, PROMPT)\u001b[0m\n\u001b[1;32m     17\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m     18\u001b[0m     answers\u001b[38;5;241m.\u001b[39mappend(answer)\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mtimer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results, answers\n",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m, in \u001b[0;36mSpeedLimitTimer.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m time_div \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord_time\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time_div \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msecond_per_step:\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msecond_per_step\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtime_div\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ChatGPT performance with tool use when prompted with instruction and 4 task demonstrations. This should give you scores 55-70%\n",
    "fourshot_results, answers = toolformer_prompt_chatgpt(test_dataset, TOOLFORMER_CALCULATOR_PROMPT_4Shot)\n",
    "evaluate(fourshot_results, answers)   # run 1 64.66,  run 2 65.33, run 3 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " inf,\n",
       " 5,\n",
       " 14,\n",
       " inf,\n",
       " 23,\n",
       " inf,\n",
       " 45.0,\n",
       " 104,\n",
       " 333,\n",
       " 93899,\n",
       " 36,\n",
       " 175,\n",
       " 660,\n",
       " 255,\n",
       " 10,\n",
       " 737.0,\n",
       " 230,\n",
       " 10,\n",
       " 3.0,\n",
       " inf,\n",
       " 3.0,\n",
       " 82,\n",
       " 111,\n",
       " 4536,\n",
       " 11,\n",
       " 614,\n",
       " 143550,\n",
       " 8,\n",
       " 18,\n",
       " 3,\n",
       " 4.0,\n",
       " 183,\n",
       " 1,\n",
       " 2825,\n",
       " 20,\n",
       " 30144,\n",
       " 527292,\n",
       " 23,\n",
       " 13,\n",
       " 30,\n",
       " 2,\n",
       " 10,\n",
       " 17017,\n",
       " 574664,\n",
       " 1088,\n",
       " 5,\n",
       " 6,\n",
       " 4.0,\n",
       " 6,\n",
       " 4140,\n",
       " 8,\n",
       " 33.0,\n",
       " inf,\n",
       " 146,\n",
       " 51,\n",
       " 2,\n",
       " 128,\n",
       " 2.0,\n",
       " 58,\n",
       " 1145,\n",
       " 84,\n",
       " 0.022727272727272728,\n",
       " 34,\n",
       " 192,\n",
       " 1,\n",
       " 302611,\n",
       " 297,\n",
       " 30057,\n",
       " 17.0,\n",
       " 7,\n",
       " 109,\n",
       " 84,\n",
       " 61,\n",
       " 174,\n",
       " 8.0,\n",
       " 337,\n",
       " 4.0,\n",
       " 94,\n",
       " 8,\n",
       " 32,\n",
       " 3,\n",
       " 51,\n",
       " 68,\n",
       " 8,\n",
       " 16,\n",
       " 16,\n",
       " 747,\n",
       " 16,\n",
       " 110,\n",
       " 8066,\n",
       " 14,\n",
       " 11,\n",
       " 7,\n",
       " 0.6666666666666666,\n",
       " 5,\n",
       " 6,\n",
       " 61,\n",
       " 31,\n",
       " -18,\n",
       " 19,\n",
       " 229,\n",
       " 1,\n",
       " 22,\n",
       " 30,\n",
       " 19,\n",
       " 1891,\n",
       " 3021,\n",
       " 3,\n",
       " 347,\n",
       " 1363293,\n",
       " 65,\n",
       " 23,\n",
       " 78,\n",
       " 76.0,\n",
       " 44,\n",
       " 17,\n",
       " 5,\n",
       " 826,\n",
       " 143,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 39,\n",
       " 1,\n",
       " 6,\n",
       " 3.0,\n",
       " 3,\n",
       " \"Error evaluating expression: '(' was never closed (<string>, line 1)\",\n",
       " 54,\n",
       " 22,\n",
       " 14,\n",
       " 20,\n",
       " 22800,\n",
       " 134,\n",
       " -4,\n",
       " 26,\n",
       " 121,\n",
       " 3,\n",
       " 648,\n",
       " 19,\n",
       " 125,\n",
       " 41,\n",
       " 223,\n",
       " 6,\n",
       " \"Error evaluating expression: '(' was never closed (<string>, line 1)\",\n",
       " 5,\n",
       " 21,\n",
       " 388,\n",
       " 460,\n",
       " 15,\n",
       " 32,\n",
       " 28,\n",
       " 342,\n",
       " 6,\n",
       " 5.0,\n",
       " 90,\n",
       " 54,\n",
       " 8,\n",
       " 7,\n",
       " 367,\n",
       " \"Error evaluating expression: '(' was never closed (<string>, line 1)\",\n",
       " 18,\n",
       " 13,\n",
       " 27,\n",
       " 4,\n",
       " 17.0,\n",
       " 5.0,\n",
       " 1.8,\n",
       " 33,\n",
       " 35,\n",
       " 111,\n",
       " 1396,\n",
       " 2,\n",
       " inf,\n",
       " 29,\n",
       " 111,\n",
       " 39,\n",
       " 84,\n",
       " 810,\n",
       " 74,\n",
       " 3.0,\n",
       " 365,\n",
       " 103,\n",
       " 17,\n",
       " 217,\n",
       " 8,\n",
       " 140,\n",
       " 2.0,\n",
       " 331,\n",
       " 2,\n",
       " 7,\n",
       " 18,\n",
       " 3,\n",
       " 2,\n",
       " 38,\n",
       " (313, 199, 865),\n",
       " 6,\n",
       " \"Error evaluating expression: '(' was never closed (<string>, line 1)\",\n",
       " 2.0,\n",
       " 4089,\n",
       " 22.0,\n",
       " 14,\n",
       " 3,\n",
       " 95,\n",
       " 22,\n",
       " 11,\n",
       " 33,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 154,\n",
       " 12,\n",
       " 17,\n",
       " 31,\n",
       " 7.0,\n",
       " 2.0,\n",
       " 5.0,\n",
       " 2.0,\n",
       " 118,\n",
       " 13,\n",
       " 125,\n",
       " 268627,\n",
       " 7,\n",
       " 5.0,\n",
       " 2,\n",
       " 8,\n",
       " 86,\n",
       " 11,\n",
       " 369,\n",
       " 9,\n",
       " 28,\n",
       " \"Error evaluating expression: '(' was never closed (<string>, line 1)\",\n",
       " -32,\n",
       " 3.0,\n",
       " 7,\n",
       " 10,\n",
       " \"Error evaluating expression: name 'Doug' is not defined\",\n",
       " \"Error evaluating expression: '(' was never closed (<string>, line 1)\",\n",
       " 30,\n",
       " 69,\n",
       " 7,\n",
       " 64,\n",
       " 117,\n",
       " 2.0,\n",
       " 314,\n",
       " 15,\n",
       " 64.0,\n",
       " 1,\n",
       " 9,\n",
       " 4,\n",
       " 57,\n",
       " 1,\n",
       " 5046,\n",
       " inf,\n",
       " -8,\n",
       " 2.0,\n",
       " 348,\n",
       " 8,\n",
       " 10,\n",
       " 0.5565217391304348,\n",
       " 3,\n",
       " 124,\n",
       " 3,\n",
       " 35,\n",
       " 14,\n",
       " 47,\n",
       " 23,\n",
       " 7,\n",
       " 6,\n",
       " 1568,\n",
       " 13,\n",
       " 7,\n",
       " 41,\n",
       " 720,\n",
       " 0.4642857142857143,\n",
       " 6.0,\n",
       " 4,\n",
       " 'Error evaluating expression: invalid syntax (<string>, line 1)',\n",
       " 14.0,\n",
       " 1,\n",
       " 19,\n",
       " 1.0,\n",
       " 2,\n",
       " 2673,\n",
       " 22,\n",
       " 1487336,\n",
       " 2,\n",
       " 1,\n",
       " 95,\n",
       " 39,\n",
       " 728,\n",
       " 30,\n",
       " 720,\n",
       " 27,\n",
       " 37.0,\n",
       " 3168,\n",
       " 60,\n",
       " 106,\n",
       " 91]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scores on the test set:  0.67\n"
     ]
    }
   ],
   "source": [
    "# ChatGPT performance with tool use when prompted with instruction and 1 task demonstrations. This should give you scores slightly lower than the 4 shot results.\n",
    "oneshot_results, answers = toolformer_prompt_chatgpt(test_dataset, TOOLFORMER_CALCULATOR_PROMPT_1Shot)  # run 1 62.66 run 2 65.33 run 3 66.33\n",
    "evaluate(oneshot_results, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Challenging Data\n",
    "#### In this section, we are going to push our models to the limit. We (naively) identify a challenging subset of the dataset by picking the math equations that involve at least one number with over 3 digits. \n",
    "#### You will observe a even larger performance difference between using tools and not using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_number(elements):\n",
    "    for element in elements:\n",
    "        try:\n",
    "            number = int(float(element))\n",
    "            if len(list(str(number))) > 3:\n",
    "                return True\n",
    "        except:\n",
    "            pass\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "challenging_dataset = []\n",
    "for data in train_dataset:     \n",
    "    elements = data[\"Equation\"].split()\n",
    "    if check_number(elements):\n",
    "        challenging_dataset.append(data)\n",
    "for data in test_dataset:     \n",
    "    elements = data[\"Equation\"].split()\n",
    "    if check_number(elements):\n",
    "        challenging_dataset.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scores on the test set:  0.6842105263157895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:59<00:00,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scores on the test set:  0.3157894736842105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "oneshot_results, answers = toolformer_prompt_chatgpt(challenging_dataset, TOOLFORMER_CALCULATOR_PROMPT_1Shot) #  # This should give you scores 55-70%\n",
    "evaluate(oneshot_results, answers) # 68%    # 63 # gpt-3.5-turbo-0125\n",
    "\n",
    "zeroshot_results, answers = zero_shot_prompt_chatgpt(challenging_dataset)\n",
    "evaluate(zeroshot_results, answers)  # This should give you scores 25-35%   % 31   # 26 # gpt-3.5-turbo-0125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval('( ( 100.0 + 3.0 ) + 7.0 )')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. (Bonus) Build your own prompt to equip LLM with calculator\n",
    "### The improvements by leveraging tool use if evident. Now, can you create a better prompt than the toolformer prompt? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_CALCULATOR_PROMPT = \"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yourprompt_results, answers = toolformer_prompt_chatgpt(test_dataset, YOUR_CALCULATOR_PROMPT)\n",
    "evaluate(yourprompt_results, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Insight Sharing \n",
    "### Please write down a short paragrpah (50 ~ 100 words) and tell us any insights you got from Part1."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed238bab1aaaef3a7cd1c28ac99be740268176041aeb0fbb1be53a9e7c2f5f2c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.16 ('DST-prompt3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
